{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "from abc import abstractmethod, ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/shakespeare.txt') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(ABC):\n",
    "    @abstractmethod\n",
    "    def build(self, data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, input):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, tokens):\n",
    "        pass\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def build(self, data, tokens):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, context, gen_steps=100):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CTokenizer(Tokenizer):\n",
    "    def build(self, data):\n",
    "        self.tokens = sorted(list(set(data)))\n",
    "        self.ttoi = { t:i for i,t  in enumerate(self.tokens)}\n",
    "        self.itot = { i:t for i,t  in enumerate(self.tokens)}\n",
    "\n",
    "    def encode(self, input):\n",
    "        return [self.ttoi[c] for c in input]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ''.join(self.itot[t] for t in tokens)\n",
    "\n",
    "class WTokenizer(Tokenizer):\n",
    "    def build(self, data):\n",
    "        self.tokens = sorted(list(set(data.split())))\n",
    "        self.ttoi = { t:i for i,t  in enumerate(self.tokens)}\n",
    "        self.itot = { i:t for i,t  in enumerate(self.tokens)}\n",
    "\n",
    "    def encode(self, input):\n",
    "        return [self.ttoi[w] for w in input.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ' '.join(self.itot[t] for t in tokens)\n",
    "\n",
    "class Model0(Model):\n",
    "    def build(self, data, tokens):\n",
    "        self.chars = sorted(list(set(data)))\n",
    "    \n",
    "    def generate(self, context, gen_steps=100):\n",
    "        output = ''.join(random.choices(self.chars, k=gen_steps))\n",
    "        return context + output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 46, 43, 1, 57, 39, 47, 42, 1, 58, 46, 47, 57, 1, 61, 39, 57, 1, 39]\n",
      "[3506, 19873, 22837, 24568, 4428]\n"
     ]
    }
   ],
   "source": [
    "test_str = 'She said this was a'\n",
    "\n",
    "c_tkzr = CTokenizer()\n",
    "c_tkzr.build(txt)\n",
    "print(c_tkzr.encode(test_str))\n",
    "\n",
    "w_tkzr = WTokenizer()\n",
    "w_tkzr.build(txt)\n",
    "print(w_tkzr.encode(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def build(self, data, tokens):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, context, gen_steps=100):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gonna have to rewrite this later, but basically you have to write different way to encode it with words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-th order approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model0(Model):\n",
    "    def build(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def generate(self, context, gen_steps=100):\n",
    "        output = random.choices(range(len((self.tokenizer.tokens))), k=gen_steps)\n",
    "        return context + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She said this was aJGnZ!o&mAokT.FpfBM3k&zYTWNA\n",
      "icB:-L'BltA;vRUazEfRELIRkYqgDWB'S;zIaBN!j,kjUPjVfiG:T'APtZueCkUY?BBe3SHP\n"
     ]
    }
   ],
   "source": [
    "m0 = Model0()\n",
    "m0.build(txt, c_tkzr)\n",
    "output = m0.generate(c_tkzr.encode(test_str))\n",
    "print(c_tkzr.decode(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-st order approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(Model):\n",
    "    def build(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        tokenized_data = self.tokenizer.encode(data)\n",
    "        cnts = Counter(tokenized_data)\n",
    "        self.w_chars = [cnts[self.tokenizer.encode(c)[0]] for c in self.tokenizer.tokens]\n",
    "    \n",
    "    def generate(self, context, gen_steps=100):\n",
    "        output = random.choices(range(len((self.tokenizer.tokens))), weights=self.w_chars, k=gen_steps)\n",
    "        return context + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She said this was as?T,a  tonaiSisvntai ed i thhBellnIbsR,agotoo.sne:noluthh:\n",
      "ifre\n",
      "o.alaTdo dibgee sio,aI,uo ryo i lelh\n"
     ]
    }
   ],
   "source": [
    "m1 = Model1()\n",
    "m1.build(txt, c_tkzr)\n",
    "output = m1.generate(c_tkzr.encode(test_str))\n",
    "print(c_tkzr.decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd order approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(Model):\n",
    "    def build(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unigram_weights = { i:[0 for _ in range(len(self.tokenizer.tokens))] for i in range(len(self.tokenizer.tokens)) }\n",
    "\n",
    "        # Process the data and create weights\n",
    "        tokenized_data = self.tokenizer.encode(data)\n",
    "        unigram_pairs = list(zip(tokenized_data, tokenized_data[1:]))\n",
    "        for fc, sc in unigram_pairs:\n",
    "            self.unigram_weights[fc][sc] += 1\n",
    "    \n",
    "    def generate(self, context, gen_steps=100):\n",
    "        \"\"\"\n",
    "        Generates new tokens based off context.\n",
    "        NOTE: For unigram, you can't start generation off of an empty string.\n",
    "        \"\"\"\n",
    "        output = context\n",
    "        for _ in range(gen_steps):\n",
    "            output.append(random.choices(range(len(self.tokenizer.tokens)), weights=self.unigram_weights[output[-1]])[0])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She said this was anje, ng Y:\n",
      "T:\n",
      "Myol es;\n",
      "ARO:--tooristhang st I cthal opung fororonoucane-dkin ard th ind s hesuthid m\n"
     ]
    }
   ],
   "source": [
    "m2 = Model2()\n",
    "m2.build(txt, c_tkzr)\n",
    "output = m2.generate(c_tkzr.encode(test_str))\n",
    "print(c_tkzr.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model0:\n",
      "She said this was a-SATW\n",
      "z&Ur3J:wfGVUVNWC$ONy-jKO'uU$v'l,mJ!htrdwBYIc\n",
      "\n",
      "\n",
      "Model1:\n",
      "She said this was a uioeeknheIB gpIoIshynstlshwm?lbobb ,Ib;, ;\n",
      "i\n",
      "dndr\n",
      "\n",
      "\n",
      "Model2:\n",
      "She said this was an's.\n",
      "\n",
      "Wie her sesel\n",
      "AREDUE ayamu pp hthisachthare \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = txt\n",
    "tokenizer = c_tkzr \n",
    "test_str = 'She said this was a'\n",
    "\n",
    "models = [Model0(), Model1(), Model2()]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.build(data, tokenizer)\n",
    "    output = model.generate(tokenizer.encode(test_str), gen_steps=50)\n",
    "    print(f'Model{i}:\\n{tokenizer.decode(output)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-tokenization and same things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[920, 6224, 7165, 7711, 1238]\n",
      "8100\n"
     ]
    }
   ],
   "source": [
    "w_tkzr = WTokenizer()\n",
    "w_tkzr.build(txt[:200_000])\n",
    "print(w_tkzr.encode(test_str))\n",
    "\n",
    "print(len(w_tkzr.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model0:\n",
      "She said this was a white again? planted Cushions, method, wise, behold men's pass'd Romans. vent comes Must, side saving senators, Ladders, covets certain themselves, Marry, requests movers fear. hell. Romans.' Brother alike. brave? natural\n",
      "\n",
      "\n",
      "Model1:\n",
      "She said this was a by BRUTUS: duke. your But it lack i' were wealsmen Capitol-- make Only in or would think stopp'd my estimation my prey angry Volsce, i' stripes mother, hang I your\n",
      "\n",
      "\n",
      "Model2:\n",
      "She said this was a humorous patrician, and parent. CORIOLANUS: I count my nature is left, Marcius: A weeder-out of wine. Second Citizen: But we the king, To your voices, I came, Ready to put\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = txt[:200_000]\n",
    "tokenizer = w_tkzr\n",
    "test_str = 'She said this was a'\n",
    "\n",
    "models = [Model0(), Model1(), Model2()]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    model.build(data, tokenizer)\n",
    "    output = model.generate(tokenizer.encode(test_str), gen_steps=30)\n",
    "    print(f'Model{i}:\\n{tokenizer.decode(output)}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
